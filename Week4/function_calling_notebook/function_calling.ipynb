{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================================\n",
    "# Jupyter Notebook: Function Calling 101\n",
    "# ====================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scenario:\n",
    "    We have 4 functions:\n",
    "      1. get_weather_info(city)\n",
    "      2. book_flight(loc_origin, loc_destination, datetime, airline)\n",
    "      3. extract_entities\n",
    "      4. tag text\n",
    "\n",
    "We present them to the OpenAI model with 'function_descriptions'.\n",
    "When the user asks a question that matches the function usage, \n",
    "the model will produce a structured function call (JSON) with name & arguments.\n",
    "\n",
    "We'll manually parse the result from the model, call our Python function, \n",
    "and then feed that result back to the model to produce a final user-facing answer.\n",
    "\n",
    "TODO:\n",
    "  - Add more interesting scenarios (like file_complaint).\n",
    "  - Experiment with 'function_call=\"auto\"' vs. forced or required.\n",
    "  - Combine multiple user requests into a single prompt to see if the model calls \n",
    "    multiple functions or just one.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "########################################################\n",
    "# Section 1: Setup\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Key Found: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We'll use environment variables to store API keys.\n",
    "Make sure you have an OPENAI_API_KEY environment variable set.\n",
    "\"\"\"\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import openai\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"OpenAI Key Found:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n",
    "\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------\n",
    "# 2) Define Python functions to be \"called\"\n",
    "# -------------------------------------------------\n",
    "In this cell, we define the **actual** Python functions the model can “call.”  \n",
    "- `get_weather_info(city)`: Returns mocked weather data (in real usage, you'd call an actual weather API).  \n",
    "- `book_flight(...)`: Pretends to book a flight and returns a JSON-formatted confirmation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_info(city: str):\n",
    "    \"\"\"\n",
    "    Dummy function that returns made-up weather data.\n",
    "    In reality, you'd call a weather API (like OpenWeatherMap).\n",
    "    \"\"\"\n",
    "    fake_data = {\n",
    "        \"Amsterdam\": {\"temp\": 15, \"condition\": \"Drizzle\"},\n",
    "        \"New York\": {\"temp\": 22, \"condition\": \"Sunny\"},\n",
    "        \"Paris\": {\"temp\": 16, \"condition\": \"Overcast\"}\n",
    "    }\n",
    "    weather = fake_data.get(city, {\"temp\": 0, \"condition\": \"Unknown\"})\n",
    "    \n",
    "    return json.dumps({\n",
    "        \"city\": city,\n",
    "        \"temperature_c\": weather[\"temp\"],\n",
    "        \"conditions\": weather[\"condition\"]\n",
    "    })\n",
    "\n",
    "def book_flight(loc_origin: str, loc_destination: str, datetime_str: str, airline: str):\n",
    "    \"\"\"\n",
    "    Dummy function to 'book' a flight.\n",
    "    In reality, you'd integrate with an airline or travel booking API.\n",
    "    \"\"\"\n",
    "    return json.dumps({\n",
    "        \"status\": \"success\",\n",
    "        \"origin\": loc_origin,\n",
    "        \"destination\": loc_destination,\n",
    "        \"datetime\": datetime_str,\n",
    "        \"airline\": airline,\n",
    "        \"confirmation_number\": \"ABC123XYZ\"\n",
    "    })\n",
    "\n",
    "def extract_entities(text: str):\n",
    "    \"\"\"\n",
    "    Dummy function that 'extracts' person names and ages from text.\n",
    "    We'll simulate the result as a simple dictionary.\n",
    "    In reality, you'd do more sophisticated NER or rely on LLM logic directly.\n",
    "    \"\"\"\n",
    "    # Very naive \"parser\"\n",
    "    # If it sees \"Joe is 30\" => we store that as an entity\n",
    "    entities = []\n",
    "    words = text.split()\n",
    "    for i, w in enumerate(words):\n",
    "        if w.lower() in [\"joe\", \"mary\", \"bob\"]:\n",
    "            # check if next words might be \"is <age>\"\n",
    "            if i+2 < len(words) and words[i+1].lower() in [\"is\"] and words[i+2].isdigit():\n",
    "                entities.append({\"name\": w.capitalize(), \"age\": int(words[i+2])})\n",
    "            else:\n",
    "                entities.append({\"name\": w.capitalize(), \"age\": None})\n",
    "    return json.dumps({\"entities\": entities})\n",
    "\n",
    "def tag_text(text: str):\n",
    "    \"\"\"\n",
    "    Dummy function for tagging text with sentiment + language.\n",
    "    In reality, you'd call a sentiment classifier or language detection library.\n",
    "    \"\"\"\n",
    "    # We'll simulate some trivial checks:\n",
    "    sentiment = \"neutral\"\n",
    "    if any(x in text.lower() for x in [\"love\", \"great\", \"amazing\"]):\n",
    "        sentiment = \"pos\"\n",
    "    elif any(x in text.lower() for x in [\"hate\", \"terrible\", \"bad\", \"dislike\"]):\n",
    "        sentiment = \"neg\"\n",
    "\n",
    "    # We'll do a naive language detection check for 'mi piace' => italian\n",
    "    language = \"en\"\n",
    "    if \"mi piace\" in text.lower():\n",
    "        language = \"it\"\n",
    "    \n",
    "    return json.dumps({\n",
    "        \"sentiment\": sentiment,\n",
    "        \"language\": language\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Describe these functions for OpenAI\n",
    "# -------------------------------------------------\n",
    "This cell simply displays all the functions (or “tools”) we’ve defined. We assign each function a name, description, and a JSON schema for its arguments, so the model knows how to call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function descriptions loaded.\n"
     ]
    }
   ],
   "source": [
    "# List of function descriptions for API calls\n",
    "function_descriptions = [\n",
    "    {\n",
    "        \"type\": \"function\",  # Defines this as a function tool\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather_info\",  # Function name used by the LLM\n",
    "            \"description\": \"Retrieve current weather information for a city.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",  # Function expects an object as input\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",  # City name as input\n",
    "                        \"description\": \"City to retrieve weather data for, e.g., 'Amsterdam'.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"city\"]  # City is mandatory\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"book_flight\",\n",
    "            \"description\": \"Book a flight between two locations with a preferred airline.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"loc_origin\": {  \n",
    "                        \"type\": \"string\",  # Departure location\n",
    "                        \"description\": \"3-letter airport code or city name of departure.\"\n",
    "                    },\n",
    "                    \"loc_destination\": {\n",
    "                        \"type\": \"string\",  # Arrival location\n",
    "                        \"description\": \"3-letter airport code or city name of arrival.\"\n",
    "                    },\n",
    "                    \"datetime\": {\n",
    "                        \"type\": \"string\",  # Date/Time in ISO format\n",
    "                        \"description\": \"Flight date/time in ISO format, e.g., '2024-06-01 08:00'.\"\n",
    "                    },\n",
    "                    \"airline\": {\n",
    "                        \"type\": \"string\",  # Airline preference\n",
    "                        \"description\": \"Preferred airline for booking.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"loc_origin\", \"loc_destination\", \"datetime\", \"airline\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"extract_entities\",\n",
    "            \"description\": \"Extract named entities (e.g., person name, age) from text.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\n",
    "                        \"type\": \"string\",  # Input text containing entities\n",
    "                        \"description\": \"Text to analyze for named entity recognition (NER).\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"text\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"tag_text\",\n",
    "            \"description\": \"Analyze text and tag it with sentiment and language.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\n",
    "                        \"type\": \"string\",  # Input text to be classified\n",
    "                        \"description\": \"Text to be tagged with sentiment and language classification.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"text\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Function descriptions loaded.\")  # Confirmation message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Define and Document the Function `test_call_model`\n",
    "\n",
    "In this cell, we create a helper function named `test_call_model`. It:\n",
    "1. Accepts a user prompt (`user_message`) and a `function_call` mode (`\"auto\"`, `\"none\"`, or `{\"name\": \"...function_name...\"}`).\n",
    "2. Calls the OpenAI API with our **function (tool) descriptions** so the model knows which functions are available.\n",
    "3. Returns the model's raw response, which may include a function call if it decides that is relevant.\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "\n",
    " -------------------------------------------------\n",
    " Quick test with 'get_weather_info'\n",
    " -------------------------------------------------\n",
    " Purpose: \n",
    " Test if the model calls the \"get_weather_info\" function \n",
    " when we ask about the weather in Amsterdam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We'll pass our function schema to the model. The model can decide\n",
    "to call one function, multiple, or none, depending on user input.\n",
    "\n",
    "- function_call=\"auto\" => The model decides if/when to call.\n",
    "- function_call=\"none\" => The model cannot call any function.\n",
    "- function_call={\"name\":\"book_flight\"} => Force it to call 'book_flight'.\n",
    "\n",
    "Try toggling these below in the 'test_call_model' function.\n",
    "\"\"\"\n",
    "\n",
    "def test_call_model(user_message: str, function_call=\"auto\"):\n",
    "    \"\"\"\n",
    "    1) We send user_message + function_descriptions to the model\n",
    "    2) We see if it returns a function call\n",
    "    3) If so, we parse arguments, call the function ourselves\n",
    "    4) Return final output\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": user_message}],\n",
    "        tools=function_descriptions,  # 'functions' is now called 'tools'\n",
    "        tool_choice=function_call,    # 'function_call' is now 'tool_choice'\n",
    "    )\n",
    "    response = completion.choices[0].message\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test with `get_weather_info`**\n",
    "Here, we try a simple user prompt asking about the weather in Amsterdam.  \n",
    "- We use `function_call=\"auto\"` so the model may decide to invoke our `get_weather_info` function if it deems it relevant.  \n",
    "- The cell prints out the raw model response so we can see if it includes a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      " ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_Y7ArYvSe8KVUtby4fWOacJHS', function=Function(arguments='{\"city\":\"Amsterdam\"}', name='get_weather_info'), type='function')])\n",
      "\n",
      "We expect a 'function_call' to get_weather_info.\n"
     ]
    }
   ],
   "source": [
    "# Let's do a quick test with something that calls 'get_weather_info'\n",
    "user_prompt_1 = \"What is the weather in Amsterdam?\"\n",
    "resp = test_call_model(user_prompt_1, function_call=\"auto\")\n",
    "print(\"Model response:\\n\", resp)\n",
    "print(\"\\nWe expect a 'function_call' to get_weather_info.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------\n",
    "# 5) Parse the response and call the function\n",
    "# -------------------------------------------------\n",
    "\n",
    " -------------------------------------------------\n",
    " Why This Code is Useful:\n",
    " Many LLM-based apps need to handle model outputs that\n",
    " specify a \"function call.\" This code acts as a \"bridge\":\n",
    " - If the LLM wants to invoke a function (to fetch data or\n",
    "   perform an action), we parse the model's tool_call info,\n",
    "   call the real Python function, and then use another API\n",
    "   call to produce a final, user-facing answer.\n",
    " - This approach is crucial for letting LLMs integrate\n",
    "   with external systems (e.g., weather APIs, booking\n",
    "   services) in a controlled, structured manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model wants to call function get_weather_info with args {'city': 'Amsterdam'}\n",
      "\n",
      "Final text back to user:\n",
      " The current weather in Amsterdam is 15°C with drizzly conditions.\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def handle_function_call(response) -> str:\n",
    "    \"\"\"\n",
    "    If the response contains a function call, execute it, return the result,\n",
    "    and pass it back to the model for a final answer.\n",
    "    \"\"\"\n",
    "    # If there's no function call, return the assistant's response as normal\n",
    "    if response.content and not hasattr(response, \"tool_calls\"):\n",
    "        return response.content\n",
    "\n",
    "    # Extract function call information\n",
    "    tool_calls = response.tool_calls\n",
    "    if not tool_calls:\n",
    "        return response.content\n",
    "\n",
    "    # Process the first function call in the list\n",
    "    tool_call = tool_calls[0]\n",
    "    fn_name = tool_call.function.name\n",
    "    fn_args = json.loads(tool_call.function.arguments)\n",
    "    tool_call_id = tool_call.id  # Extract tool_call_id\n",
    "    print(f\"Model wants to call function {fn_name} with args {fn_args}\")\n",
    "\n",
    "    # Route to the correct function\n",
    "    if fn_name == \"get_weather_info\":\n",
    "        city_req = fn_args[\"city\"]\n",
    "        function_result = get_weather_info(city_req)\n",
    "\n",
    "    elif fn_name == \"book_flight\":\n",
    "        loc_origin = fn_args[\"loc_origin\"]\n",
    "        loc_dest = fn_args[\"loc_destination\"]\n",
    "        dt = fn_args[\"datetime\"]\n",
    "        airline = fn_args[\"airline\"]\n",
    "        function_result = book_flight(loc_origin, loc_dest, dt, airline)\n",
    "    elif fn_name == \"extract_entities\":\n",
    "        text = fn_args[\"text\"]\n",
    "        function_result = extract_entities(text)\n",
    "    elif fn_name == \"tag_text\":\n",
    "        text = fn_args[\"text\"]\n",
    "        function_result = tag_text(text)\n",
    "    else:\n",
    "        return \"Function not recognized\"\n",
    "\n",
    "    # Ensure function_result is a string\n",
    "    function_result = str(function_result)\n",
    "\n",
    "    # Handle possible None content\n",
    "    previous_content = f\"Previously: {response.content}\" if response.content else \"Processing function call...\"\n",
    "\n",
    "    # Make a second model request, providing the function result\n",
    "    # so it can finalize a user-facing response. \n",
    "    # This step is commonly used to incorporate the function's data\n",
    "    # (e.g., \"temp=15C, partly cloudy\") into a full natural language\n",
    "    # answer for the end user.\n",
    "    second_response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": previous_content},  \n",
    "            {\"role\": \"assistant\", \"tool_calls\": response.tool_calls},  # Include the original tool call\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call_id,\n",
    "                \"name\": fn_name,\n",
    "                \"content\": function_result\n",
    "            }  \n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Extract final answer\n",
    "    final_answer = second_response.choices[0].message.content\n",
    "    return final_answer\n",
    "\n",
    "# Test it end-to-end\n",
    "final_text = handle_function_call(resp)\n",
    "print(\"\\nFinal text back to user:\\n\", final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------\n",
    "# 6) Another Example: Book a flight\n",
    "# -------------------------------------------------\n",
    " Why This Is Useful:\n",
    " Demonstrates how a user request can trigger the \"book_flight\" function.\n",
    " This pattern can generalize to many use cases like \"place an order\",\n",
    " \"schedule a meeting\", etc. The LLM decides the best function to call\n",
    " and we do the behind-the-scenes work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      " ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_Utqaotqsle9QD4xkxazgPym7', function=Function(arguments='{\"loc_origin\":\"AMS\",\"loc_destination\":\"JFK\",\"datetime\":\"2024-06-02T10:00:00\",\"airline\":\"Delta Airlines\"}', name='book_flight'), type='function')])\n",
      "Model wants to call function book_flight with args {'loc_origin': 'AMS', 'loc_destination': 'JFK', 'datetime': '2024-06-02T10:00:00', 'airline': 'Delta Airlines'}\n",
      "\n",
      "User-Facing Answer:\n",
      " Your flight from Amsterdam (AMS) to New York JFK (JFK) with Delta Airlines on June 2, 2024, at 10:00 AM has been successfully booked. Your confirmation number is **ABC123XYZ**. Safe travels!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We'll ask a multi-part question that might trigger the model to call the second function.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_2 = \"I want to book a flight from AMS to JFK on 2024-06-02 at 10:00 with Delta Airlines\"\n",
    "resp2 = test_call_model(user_prompt_2, function_call=\"auto\")\n",
    "print(\"Model response:\\n\", resp2)\n",
    "\n",
    "final_text_2 = handle_function_call(resp2)\n",
    "print(\"\\nUser-Facing Answer:\\n\", final_text_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------\n",
    "# 7)  Section: Entity Extraction\n",
    "# -------------------------------------------------\n",
    " Why This Is Useful:\n",
    " Shows how the LLM can parse unstructured text (e.g., \"Joe is 30,\n",
    " Mary is older...\") and call a function that extracts relevant entities\n",
    " or structured data. This is a building block for advanced data extraction,\n",
    " knowledge-base population, or record creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Entity Extraction Test ===\n",
      "Model response:\n",
      " ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_hoNtRO8MJTQPzxKyTXASf3UX', function=Function(arguments='{\"text\":\"Joe is 30, Mary is older but we don\\'t know her age.\"}', name='extract_entities'), type='function')])\n",
      "Model wants to call function extract_entities with args {'text': \"Joe is 30, Mary is older but we don't know her age.\"}\n",
      "\n",
      "User-Facing Answer:\n",
      " The entities extracted from the text are:\n",
      "- Joe, age is not specified.\n",
      "- Mary, age is not specified but she is older than Joe.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We have an 'extract_entities' function that looks for simple name/age pairs.\n",
    "Let's see if the LLM picks that function for a user query describing people.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_3 = \"Joe is 30, Mary is older but we don't know her age.\"\n",
    "resp3 = test_call_model(user_prompt_3, function_call=\"auto\")\n",
    "print(\"\\n=== Entity Extraction Test ===\")\n",
    "print(\"Model response:\\n\", resp3)\n",
    "final_text_3 = handle_function_call(resp3)\n",
    "print(\"\\nUser-Facing Answer:\\n\", final_text_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------\n",
    "# 8) Section: Tagging\n",
    "# -------------------------------------------------\n",
    " Why This Is Useful:\n",
    " Tagging or classification can be critical for sentiment analysis,\n",
    " content moderation, or language detection. By letting the LLM call\n",
    " a specialized \"tag_text\" function, you can unify your external logic\n",
    " (like a custom sentiment model) with the language reasoning of the LLM.\n",
    "\n",
    " (Implementation of the function or usage example would go here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tagging Test ===\n",
      "Model response:\n",
      " ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_i2Ch1kzmSNfTjjUfTgYgQz2J', function=Function(arguments='{\"text\":\"mi piace la pizza, it\\'s amazing!\"}', name='tag_text'), type='function')])\n",
      "Model wants to call function tag_text with args {'text': \"mi piace la pizza, it's amazing!\"}\n",
      "\n",
      "User-Facing Answer:\n",
      " You expressed a positive sentiment about pizza in Italian!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We have a 'tag_text' function that returns a naive sentiment & language.\n",
    "We'll see if the model calls it automatically if the user requests tagging.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_4 = \"Can you tag this text for me: 'mi piace la pizza, it's amazing!'\"\n",
    "resp4 = test_call_model(user_prompt_4, function_call=\"auto\")\n",
    "print(\"\\n=== Tagging Test ===\")\n",
    "print(\"Model response:\\n\", resp4)\n",
    "final_text_4 = handle_function_call(resp4)\n",
    "print(\"\\nUser-Facing Answer:\\n\", final_text_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------\n",
    "# 9) Activity / Optional Challenge\n",
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Entity Extraction Test ===\n",
      "Model response:\n",
      " ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_MiPqRW5iuVmYGBfHm3i914xe', function=Function(arguments='{\"city\": \"New York\"}', name='get_weather_info'), type='function'), ChatCompletionMessageToolCall(id='call_adoLcIcIS8aOHQFtwb37HH5R', function=Function(arguments='{\"loc_origin\": \"LAX\", \"loc_destination\": \"SFO\", \"datetime\": \"2024-11-10T09:00:00\", \"airline\": \"United\"}', name='book_flight'), type='function')])\n",
      "Model wants to call function get_weather_info with args {'city': 'New York'}\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_adoLcIcIS8aOHQFtwb37HH5R\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Entity Extraction Test ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, resp5)\n\u001b[0;32m---> 21\u001b[0m final_text_5 \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_function_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUser-Facing Answer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, final_text_5)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m2) Create a third function, e.g. \"file_complaint(name, email, text)\", \u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m   to simulate a user wanting to file a complaint about their flight.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mHave fun and experiment!\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 56\u001b[0m, in \u001b[0;36mhandle_function_call\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     49\u001b[0m previous_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreviously: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing function call...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Make a second model request, providing the function result\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# so it can finalize a user-facing response. \u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# This step is commonly used to incorporate the function's data\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# (e.g., \"temp=15C, partly cloudy\") into a full natural language\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# answer for the end user.\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m second_response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_content\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massistant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_calls\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Include the original tool call\u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_call_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_call_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_result\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Extract final answer\u001b[39;00m\n\u001b[1;32m     71\u001b[0m final_answer \u001b[38;5;241m=\u001b[39m second_response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/Documents/Jupyter/MDST/WN25 /LLM-Augmentation-W25/env/lib/python3.12/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Jupyter/MDST/WN25 /LLM-Augmentation-W25/env/lib/python3.12/site-packages/openai/resources/chat/completions.py:863\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    860\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    861\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    862\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Jupyter/MDST/WN25 /LLM-Augmentation-W25/env/lib/python3.12/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/Jupyter/MDST/WN25 /LLM-Augmentation-W25/env/lib/python3.12/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Jupyter/MDST/WN25 /LLM-Augmentation-W25/env/lib/python3.12/site-packages/openai/_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1073\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_adoLcIcIS8aOHQFtwb37HH5R\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1) Try combining the two tasks into one user request:\n",
    "   e.g. \"What's the weather in New York, \n",
    "         and also please book me a flight from LAX to SFO next Friday at 9am with United.\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_5 = \"What's the weather in New York, and also please book me a flight from LAX to SFO next Friday at 9am with United.\"\n",
    "\n",
    "\"\"\"\n",
    "   Observe if the model tries to call multiple functions or just one. \n",
    "   Because the standard function calling only returns ONE call at a time, \n",
    "   you might get partial coverage. \n",
    "   (HINT: you'll need to loop to handle multiple calls or do more advanced logic.)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "resp5 = test_call_model(user_prompt_5, function_call=\"auto\")\n",
    "print(\"\\n=== Entity Extraction Test ===\")\n",
    "print(\"Model response:\\n\", resp5)\n",
    "final_text_5 = handle_function_call(resp5)\n",
    "print(\"\\nUser-Facing Answer:\\n\", final_text_5)\n",
    "\n",
    "\"\"\"\n",
    "2) Create a third function, e.g. \"file_complaint(name, email, text)\", \n",
    "   to simulate a user wanting to file a complaint about their flight.\n",
    "   Add that to function_descriptions, \n",
    "   then see if the model picks it up when the user says \n",
    "   \"I want to file a complaint about my missed flight. My name is Jane, email is jane@example.com\"\n",
    "\n",
    "\n",
    "\n",
    "3) Experiment with forcing a function call:\n",
    "   - function_call=\"none\": The model won't produce any function calls.\n",
    "   - function_call={\"name\":\"book_flight\"}: The model *must* call the 'book_flight' function, \n",
    "     which might lead to it guessing arguments if the user didn't specify them.\n",
    "\n",
    "4) If you want an advanced challenge, \n",
    "   handle repeated calls automatically:\n",
    "   - If the model calls one function, you feed the result, \n",
    "     then it calls a second function, etc.\n",
    "   - This is sometimes referred to as a \"multi-step\" or \"agentic\" approach.\n",
    "\n",
    "Have fun and experiment!\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------\n",
    "# 10) End\n",
    "# -------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
