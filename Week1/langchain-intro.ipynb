{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will cover the very basics of LangChain. This includes chains and templates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import necessary libraries, and use `dotenv` to load our OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every chat, LLMs are typically first introduced to a \"system message,\" instructing the LLM on how to interpret the conversation. There is also a \"human\" or \"user\" message which is simply what the user sends to the LLM. An \"assistant\" or \"AI\" message is associated with the messages that the LLM itself writes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To beat Minecraft, you need to defeat the Ender Dragon, which is the final boss in the game. Here are the general steps to accomplish this:\n",
      "\n",
      "1. Gather resources: Start by collecting essential resources like wood, food, and ores to craft tools, weapons, and armor.\n",
      "\n",
      "2. Build a base: Create a safe shelter to protect yourself from hostile mobs during the night and store your items.\n",
      "\n",
      "3. Explore and mine: Venture into caves to mine valuable resources like diamonds, gold, and iron to improve your gear.\n",
      "\n",
      "4. Obtain Ender Pearls: Defeat Endermen to collect Ender Pearls, which are needed to locate and activate the End Portal.\n",
      "\n",
      "5. Find and activate the End Portal: Locate a stronghold by using Eyes of Ender and activate the End Portal by placing Eyes of Ender in the portal frames.\n",
      "\n",
      "6. Enter the End: Jump into the End Portal to enter the End dimension where the Ender Dragon resides.\n",
      "\n",
      "7. Defeat the Ender Dragon: Destroy the End Crystals that heal the Ender Dragon and then defeat the dragon by attacking its weak points until it is defeated.\n",
      "\n",
      "8. Collect the Dragon Egg: Once the Ender Dragon is defeated, collect the Dragon Egg as a trophy.\n",
      "\n",
      "9. Return home: Jump into the End Gateway portal to return to the Overworld.\n",
      "\n",
      "10. Celebrate: Congratulations! You have beaten Minecraft by defeating the Ender Dragon and completing the game's main objective.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert Minecraft player.\"),\n",
    "    HumanMessage(content=\"What do you have to do to beat Minecraft?\")\n",
    "]\n",
    "\n",
    "response = llm(messages=messages, temperature=0.6)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of the \"temperature\" parameter. Temperature is a parameter that refers to the probability with which the LLM's underlying next token predictor picks out a next token that is not the highest probability token. You can consider to it a proxy for \"creativity.\" Higher temperature -> more randomness -> more \"creativity.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent prompts using \"prompt templates,\" which allow us to dynamically plug things into prompts. The `PromptTemplate` class is simply LangChain's object interface with prompts. Chains allow us to link prompts together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Explain {topic} in one sentence\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/wll0v6yx3s741r06wmqlyvwc0000gn/T/ipykernel_92870/1723417862.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(prompt=prompt, llm=llm)\n",
      "/var/folders/zr/wll0v6yx3s741r06wmqlyvwc0000gn/T/ipykernel_92870/1723417862.py:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain.run(topic=\"atitude\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Attitude is a person's perspective, beliefs, and feelings towards a particular situation or individual.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "chain.run(topic=\"atitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A starchy tuberous crop that is widely cultivated for its edible roots.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"ml_topic_desc\"],\n",
    "    template=\"\"\"\n",
    "    You are a:\n",
    "    {ml_topic_desc}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "chain.run(topic = \"potato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs of the first chain are passed in to the second chain as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mAn autoencoder is a type of artificial neural network that learns to compress and then reconstruct data, often used for dimensionality reduction and feature learning.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Autoencoder.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Autoencoder.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "blog_post = overall_chain.run(input=\"autoencoder\")\n",
    "print(blog_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
